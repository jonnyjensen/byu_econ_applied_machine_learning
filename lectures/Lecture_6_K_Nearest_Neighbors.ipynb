{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## K-Nearest Neighbors\n",
    "\n",
    "This is one of the simpliest and easiest to understand algorithms. It can be used for both classification and regression tasks, but is more common in classification, so we will focus there. The principles, though, can be used in both cases and sklearn supports both.\n",
    "\n",
    "Basically, here is the algorithm:\n",
    "\n",
    "1. Define k\n",
    "2. Define a distance metric - usually euclidian distance\n",
    "3. For a new data point, find the k nearest training points and combine their classes in some way - usually voting - to get a predicted class\n",
    "\n",
    "That's it!\n",
    "\n",
    "**Some of the benefits:**\n",
    "\n",
    "* Doesn't really require any training in the traditional sense. You just need a fast way to find the nearest neighbors\n",
    "* Easy to understand \n",
    "\n",
    "**Some of the negatives:**\n",
    "\n",
    "* Need to define k, which is a hyper-parameter, so can be tuned with cross-validation. A higher value for k increases bias and a lower value increases variance.\n",
    "* Have to choose a distance metric and could get very different results depending on the metric. Again can use cross-validation.\n",
    "* Doesn't really offer insights into which features might be important.\n",
    "* Can suffer with high dimensional data due to the curse of dimensionality.\n",
    "\n",
    "**Basic assumption:**\n",
    "\n",
    "* Data points that are close are similar for our target\n",
    "\n",
    "\n",
    "### Curse of Dimensionality\n",
    "\n",
    "Basically, the curse of dimensionality means that in high dimensions, it is likely that close points are not much closer than the average distance, which means being close doesnt mean much. In high dimensions, the data becomes very spread out, which creates this phenomenon. \n",
    "\n",
    "There are so many good resources for this online, that I won't go any deeper. Here is one you might look at:\n",
    "\n",
    "http://blog.galvanize.com/learn/learn-to-code/curse-dimensionality-manage/\n",
    "\n",
    "### Euclidian Distance\n",
    "\n",
    "For vectors, q and p that are being compared (these would be our feature vectors):\n",
    "\n",
    "$$\\sqrt{\\sum_{i=1}^{N}{(p_{i} - q_{i})^2}}$$\n",
    "\n",
    "\n",
    "### SKlearn Example\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.datasets import load_breast_cancer\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV\n",
    "from sklearn.metrics import f1_score, classification_report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best Params: {'n_neighbors': 5, 'p': 1, 'weights': 'uniform'}\n",
      "Train F1: 0.9606837606837607\n",
      "Test Classification Report:\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.97      0.88      0.93        43\n",
      "          1       0.93      0.99      0.96        71\n",
      "\n",
      "avg / total       0.95      0.95      0.95       114\n",
      "\n"
     ]
    }
   ],
   "source": [
    "data = load_breast_cancer()\n",
    "X_train, X_test, y_train, y_test = train_test_split(data.data, data.target, test_size=0.20, random_state=42)\n",
    "clf = KNeighborsClassifier()\n",
    "gridsearch = GridSearchCV(clf, {\"n_neighbors\": [1, 3, 5, 7, 9, 11], \"weights\": ['uniform', 'distance'], \n",
    "                                'p': [1, 2, 3]}, scoring='f1')\n",
    "gridsearch.fit(X_train, y_train)\n",
    "print(\"Best Params: {}\".format(gridsearch.best_params_))\n",
    "print(\"Train F1: {}\".format(f1_score(y_train, gridsearch.predict(X_train))))\n",
    "print(\"Test Classification Report:\")\n",
    "print(classification_report(y_test, gridsearch.predict(X_test)))"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [Anaconda3]",
   "language": "python",
   "name": "Python [Anaconda3]"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
